import os
import json
from pathlib import Path
from bs4 import BeautifulSoup
import nltk.tokenize
from nltk.stem import PorterStemmer
import re
nltk.download('punkt_tab')
## TODO: should porterstem words

ind_size = 10000
partial_index_directory = os.path.join(os.getcwd(), "partial_index")
complete_index_directory = os.path.join(os.getcwd(), "complete_index")
stemmer = PorterStemmer()
complete_index_file = "complete_index.json"

def json_files(path):
    files = []
    path = Path(path)
    for json_file in path.rglob("*.json"):
        print(json_file)
        with json_file.open("r") as f:
            files.append(json.load(f))

    # for folder in os.listdir(path):
    #     folder_path = os.path.join(path, folder)
    #     if os.path.isdir(folder_path):
    #         for file in os.listdir(folder_path):
    #             if file.endswith(".json"):
    #                 file_path = os.path.join(folder_path, file)
    #                 print(file_path)
    #                 with open(file_path, "r") as f:
    #                     files.append(json.load(f))

    return files

def tokenize(text):
    word_dict = {}
    text = text.split("\n")
    for te in text:
        doc_words = BeautifulSoup(te, 'html.parser').get_text()
        # tokens = doc_words.split()
        # print(doc_words)
        tokens = nltk.word_tokenize(doc_words)
        # tokens = re.sub(r"[^\w\s]", "", token)
        for token in tokens:
            token = re.sub(r"[^\w\s]", "", token).lower()
            if token != '':
                token = stemmer.stem(token)
                if token not in word_dict:
                    word_dict[token] = 1
                else:
                    word_dict[token] += 1

        # print(word_dict)
    return word_dict

def index(files):
    index = {}

    counter = 0
    running_count = 0
    part = 1
    for doc in files:
        print(doc['url'])
        content = doc['content']
        tokens = tokenize(content)
        # print(tokens)
        print()
        for word in tokens:
            index_list = [running_count, doc['url'], tokens[word]] ## document ID, document URL, frequency of token
            if word not in index:
                index[word] = [index_list]
            else:
                index[word] += [index_list]

        # print(index)
        # time.sleep(5)
        counter += 1
        running_count += 1
        print(counter)
        if counter > ind_size: ## gets called every 10k pages, could lower i think theres like
            index_partial(index, part) ## 50k total ?
            index.clear()
            part += 1
            counter = 0
    if len(index.keys()) != 0:
        # part += 1
        index_partial(index, part) ## catches the final indexes

def index_partial(index, part):
    if not os.path.exists(partial_index_directory): ## wait is this supposed to be ran on lab or local? does os.path work for lab
        os.makedirs(partial_index_directory) ## even so need to upload all the files to the repo which is hmmmm
    filename = f"partial_index_part{part}.json"
    file = os.path.join(partial_index_directory, filename)
    with open(file, "w") as f:
        json.dump(index, f)

def index_complete():
    complete_index = {}
    partial_files = []
    for f in os.listdir(partial_index_directory):
        if f.endswith(".json"):
            partial_files.append(f) ## adding all files to a list
    
    for f in partial_files:
        with open(os.path.join(partial_index_directory, f), 'r') as f:
            partial_index = json.load(f)
            for word, info in partial_index.items():
                if word not in complete_index:
                    complete_index[word] = []
                complete_index[word] += [info]
            ### continue continue continue
    
    return complete_index
def write_complete_index(complete_index):
    af_words = 'abcdef' ## omg this looks really clunky
    gk_words = 'ghijk'
    lp_words = 'lmnop'
    qu_words = 'qrstu'
    vz_words = 'vwxyz'
    ## actually can you nametuple this ? 
    af_dict = {}
    gk_dict = {}
    lp_dict = {}
    qu_dict = {}
    vz_dict = {}
    nonalpha_dict = {}


    comp_index_sorted = dict(sorted(complete_index.items()))
    for word, info in comp_index_sorted.items():
        if any (word.startswith(letter) for letter in af_words):
            # file = os.path.join(os.getcwd(), "complete_index_af.json")
            if word not in af_dict:
                af_dict[word] = []
            af_dict[word] += info

        elif any (word.startswith(letter) for letter in gk_words):
            if word not in gk_dict:
                gk_dict[word] = []
            gk_dict[word] += info

        elif any (word.startswith(letter) for letter in lp_words):
            if word not in lp_dict:
                lp_dict[word] = []
            lp_dict[word] += info

        elif any (word.startswith(letter) for letter in qu_words):
            if word not in qu_dict:
                qu_dict[word] = []
            qu_dict[word] += info

        elif any (word.startswith(letter) for letter in vz_words):
            if word not in vz_dict:
                vz_dict[word] = []
            vz_dict[word] += info

        else:
            if word not in nonalpha_dict:
                nonalpha_dict[word] = []
            nonalpha_dict[word] += info


    af_file = os.path.join(complete_index_directory, "complete_index_af.json")
    write_files(af_file, af_dict)
## there has to be a better way skull TODO: THINK OF A BETTER WAY
    gk_file = os.path.join(complete_index_directory, "complete_index_gk.json")
    write_files(gk_file, gk_dict)

    lp_file = os.path.join(complete_index_directory, "complete_index_lp.json")
    write_files(lp_file, lp_dict)

    qu_file = os.path.join(complete_index_directory, "complete_index_qu.json")
    write_files(qu_file, qu_dict)

    vz_file = os.path.join(complete_index_directory, "complete_index_vz.json")
    write_files(vz_file, vz_dict)

    ## nonalpha is messed up because it sees a bunch of numbers like an isbn and then strips the hyphens is that supposed to happen ? 
                # so 123-456 => 123456
    nonalpha_file = os.path.join(complete_index_directory, "complete_index_nonalpha.json")
    write_files(nonalpha_file, nonalpha_dict)

    total_tokens = len(af_dict) + len(gk_dict) + len(lp_dict) + len(qu_dict) + len(vz_dict) + len(nonalpha_dict)
    return total_tokens

def write_files(file, part_complete_index):
    if not os.path.exists(complete_index_directory): ## wait is this supposed to be ran on lab or local? does os.path work for lab
        os.makedirs(complete_index_directory)
    with open(file, "w") as f:
        json.dump(part_complete_index, f)

def write_report(total_tokens, total_files):
    report_path = os.path.join(os.getcwd(), "report.txt")
    with open(report_path, "w") as f:
        msg = f"Total Number of Tokens: {total_tokens}\nTotal Number of Files: {total_files}"
        f.write(msg)


def main(path):
    files = json_files(path)
    index(files)
    complete_index = index_complete()
    total_tokens = write_complete_index(complete_index)
    write_report(total_tokens, len(files))

if __name__ == "__main__":
    path = "c:/users/16264/desktop/developer/DEV"
    main(path)
